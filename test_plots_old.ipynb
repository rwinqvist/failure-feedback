{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from solvers import ValueIteration\n",
    "from rocky_road import RockyRoad\n",
    "from env_wrapper import EnvironmentWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_policy(env, policy):\n",
    "    severities = [i for i in range(env.max_acc_sev)]\n",
    "    actions, action_names = env.actions, env.action_names\n",
    "    action_data = {f\"{action_names[action]}\": np.zeros(len(severities)) for action in actions}\n",
    "\n",
    "    # bar chart\n",
    "    for state, action in policy.items(): \n",
    "        if state not in env.goal_states and state not in env.terminal_states: \n",
    "            env_state, sev_lvl = state\n",
    "            sev_idx = severities.index(sev_lvl) \n",
    "            act_name = action_names[action]\n",
    "            action_data[act_name][sev_idx] += 1\n",
    "\n",
    "\n",
    "\n",
    "# bar plot of policy\n",
    "def plot_policy_bar(env, policy):\n",
    "    env_states = []\n",
    "    sevs = []\n",
    "    for state in env.states: \n",
    "        env_state, sev = state \n",
    "        if env_state not in env_states: \n",
    "            env_states.append(env_state)\n",
    "        if sev not in sevs: \n",
    "            sevs.append(sev)\n",
    "\n",
    "    colors = ['g', 'y', 'c', 'b', 'r']\n",
    "    yticks = sevs \n",
    "    xticks = env_states\n",
    "\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlabel('Environmental state')\n",
    "    ax.set_ylabel('Severity')\n",
    "    ax.set_zlabel('Action')\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_zticks([0,1])\n",
    "\n",
    "    data = {action: [] for action in env.actions}\n",
    "    for state, action in policy.items():\n",
    "        if state in policy and state not in env.goal_states and state not in env.terminal_states:  \n",
    "            data[action].append(state)\n",
    "\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for action in env.actions: \n",
    "        states = data[action]\n",
    "        if len(states) != 0:\n",
    "            xs = [state[0] for state in states]\n",
    "            ys = np.ones(len(xs))\n",
    "            zs = [state[1] for state in states]\n",
    "            cs = [colors[action]] * len(xs)\n",
    "\n",
    "            ax.bar(xs, ys, zs, zdir='y', \n",
    "                color=cs, alpha=0.8, label=f\"Action {action}\")\n",
    "        \n",
    "    #ax.legend([f\"Action {i}\" for i in env.actions])\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# scatter plot of policy\n",
    "def plot_policy_scatter(env, policy):\n",
    "    env_states = []\n",
    "    sevs = []\n",
    "    for state in env.states: \n",
    "        env_state, sev = state \n",
    "        if env_state not in env_states: \n",
    "            env_states.append(env_state)\n",
    "        if sev not in sevs: \n",
    "            sevs.append(sev)\n",
    "\n",
    "    colors = ['g', 'y', 'c', 'b', 'r']\n",
    "\n",
    "    xticks = sevs \n",
    "    yticks = [state[0][1] for state in env.states]\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(8,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_ylabel('Environmental state')\n",
    "    ax.set_xlabel('Severity')\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_xticks(xticks)\n",
    "\n",
    "    data = {action: [] for action in env.actions}\n",
    "    for state, action in policy.items(): \n",
    "        if state in policy and state not in env.goal_states and state not in env.terminal_states: \n",
    "            data[action].append(state)\n",
    "\n",
    "    for action in env.actions: \n",
    "        states = data[action]\n",
    "        if len(states) != 0: \n",
    "            xs = [state[1] for state in states]\n",
    "            ys = [state[0][1] for state in states]\n",
    "            cs = [colors[action]] * len(xs) \n",
    "\n",
    "            ax.scatter(xs, ys, c=cs, alpha=1, s=100, label=f\"Action {action}\")\n",
    "\n",
    "    #ax.legend([f\"Action {i}\" for i in env.actions])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "    #ax.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'cost': 16, 'success_rate': 0.65, 'performance_decline_factor': 1, 'skew_factor': 1, 'direction': 0}\n",
      "1 {'cost': 8, 'success_rate': 0.6174999999999999, 'performance_decline_factor': 1, 'skew_factor': 1, 'direction': 0}\n",
      "2 {'cost': 4, 'success_rate': 0.586625, 'performance_decline_factor': 1, 'skew_factor': 1, 'direction': 0}\n",
      "3 {'cost': 2, 'success_rate': 0.5572937499999999, 'performance_decline_factor': 1, 'skew_factor': 1, 'direction': 0}\n",
      "4 {'cost': 1, 'success_rate': 0.5294290625, 'performance_decline_factor': 1, 'skew_factor': 1, 'direction': 0}\n",
      "[[b'P' b'P' b'L' b'R' b'P' b'L' b'R' b'P' b'P' b'R' b'L' b'R' b'R' b'R'\n",
      "  b'R' b'R' b'R' b'P' b'P' b'L']\n",
      " [b'S' b'A' b'A' b'A' b'A' b'A' b'A' b'A' b'A' b'A' b'A' b'A' b'A' b'A'\n",
      "  b'A' b'A' b'A' b'A' b'A' b'G']\n",
      " [b'R' b'L' b'P' b'P' b'P' b'P' b'P' b'L' b'R' b'P' b'L' b'R' b'L' b'L'\n",
      "  b'P' b'R' b'R' b'L' b'L' b'L']]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RockyRoad' object has no attribute 'board'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 165\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key, item)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(rr\u001b[38;5;241m.\u001b[39mmap)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m)\n\u001b[1;32m    167\u001b[0m vi \u001b[38;5;241m=\u001b[39m ValueIteration(env)\n\u001b[1;32m    168\u001b[0m V, Q, policy \u001b[38;5;241m=\u001b[39m vi\u001b[38;5;241m.\u001b[39mrun()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RockyRoad' object has no attribute 'board'"
     ]
    }
   ],
   "source": [
    "ENV_INFO =  {   \n",
    "    \"length\": 20,\n",
    "    \"terrains\": [\"A\", \"C\", \"R\", \"L\", \"P\"],\n",
    "    \"severity_levels\": [1, 2, 3, 4, 5],\n",
    "    \"num_actions\": 3,\n",
    "    \"action_costs\": [1, 3, 6],\n",
    "    \"nom_success_rate\": 0.75,\n",
    "    \"risky_decline_factor\": 0.9,\n",
    "    \"severity_decline_factor\": 1,\n",
    "    \"skew_factor\": 1,\n",
    "    \"goal_reward\": 200,\n",
    "\n",
    "    \"A\": {\n",
    "        \"type\": \"allowed\",\n",
    "        \"p\": 1,\n",
    "    },\n",
    "\n",
    "    \"C\": {\n",
    "        \"type\": \"allowed\",\n",
    "        \"p\": 0,\n",
    "    },\n",
    "\n",
    "    \"R\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        #\"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "        \"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    },\n",
    "\n",
    "    \"L\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        \"nom_sev_probs\": [0.25, 0.25, 0.2, 0.2, 0.1],\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        #\"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    },\n",
    "\n",
    "    \"P\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        \"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    }\n",
    "}\n",
    "\n",
    "ENV_INFO =  {   \n",
    "    \"length\": 10,\n",
    "    \"terrains\": [\"A\", \"C\", \"R\", \"L\", \"P\"],\n",
    "    \"severity_levels\": [1, 2, 3, 4, 5],\n",
    "    \"num_actions\": 5,\n",
    "    \"action_costs\": [1, 2, 3, 4, 5],\n",
    "    #\"action_costs\": [1, 3, 6, 10, 15],\n",
    "    #\"action_costs\": [3, 7, 12, 18, 25],\n",
    "    \"nom_success_rate\": 0.9, #0.9 good, #0.8 pretty good too\n",
    "    \"risky_decline_factor\": 0.9,\n",
    "    \"severity_decline_factor\": 1,\n",
    "    \"skew_factor\": 2,\n",
    "    \"goal_reward\": 100,\n",
    "\n",
    "    \"A\": {\n",
    "        \"type\": \"allowed\",\n",
    "        \"p\": 1,\n",
    "    },\n",
    "\n",
    "    \"C\": {\n",
    "        \"type\": \"allowed\",\n",
    "        \"p\": 0,\n",
    "    },\n",
    "\n",
    "    \"R\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        \"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    },\n",
    "\n",
    "    \"L\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        \"nom_sev_probs\": [0.25, 0.25, 0.2, 0.2, 0.1],\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        #\"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    },\n",
    "\n",
    "    \"P\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        \"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "EXP_INFO = {\n",
    "    \"max_acc_sev\": 8,\n",
    "    \"query_cost\": 1,\n",
    "    \"severity_penalty_weight\": 0.1,\n",
    "    \"recovery_rate\": 0,\n",
    "    \"true_obs_prob\": 0.2,\n",
    "}\n",
    "\n",
    "ENV_INFO =  {   \n",
    "    \"length\": 20,\n",
    "    \"terrains\": [\"A\", \"C\", \"R\", \"L\", \"P\"],\n",
    "    \"severity_levels\": [1, 2, 3, 4, 5],\n",
    "    \"num_actions\": 5,\n",
    "    \"action_costs\": 1*[1, 2, 4, 8, 16],\n",
    "    #\"action_costs\": [1, 3, 6, 10, 15],\n",
    "    #\"action_costs\": [3, 7, 12, 18, 25],\n",
    "    \"nom_success_rate\": 0.65, #0.9 good, #0.8 pretty good too\n",
    "    \"risky_decline_factor\": 0.95,\n",
    "    \"severity_decline_factor\": 1,\n",
    "    \"skew_factor\": 1,\n",
    "    \"goal_reward\": 1000,\n",
    "\n",
    "    \"A\": {\n",
    "        \"type\": \"allowed\",\n",
    "        \"p\": 1,\n",
    "    },\n",
    "\n",
    "    \"C\": {\n",
    "        \"type\": \"allowed\",\n",
    "        \"p\": 0,\n",
    "    },\n",
    "\n",
    "    \"R\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        #\"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "        \"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    },\n",
    "\n",
    "    \"L\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        \"nom_sev_probs\": [0.25, 0.25, 0.2, 0.2, 0.1],\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        #\"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    },\n",
    "\n",
    "    \"P\": {\n",
    "        \"type\": \"forbidden\",\n",
    "        \"p\": 1/3,\n",
    "        #\"nom_sev_probs\": [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        \"nom_sev_probs\": [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "EXP_INFO = {\n",
    "    \"max_acc_sev\": 22,\n",
    "    \"query_cost\": 1,\n",
    "    \"severity_penalty_weight\": 0.25,\n",
    "    \"recovery_rate\": 0,\n",
    "    \"true_obs_prob\": 0.2,\n",
    "}\n",
    "\n",
    "\n",
    "rr = RockyRoad(environment_info=ENV_INFO)\n",
    "env = EnvironmentWrapper(rr, exp_info=EXP_INFO)\n",
    "for key, item in rr.action_info.items():\n",
    "    print(key, item)\n",
    "\n",
    "print(rr.map)\n",
    "print(rr.board)\n",
    "\n",
    "vi = ValueIteration(env)\n",
    "V, Q, policy = vi.run()\n",
    "\n",
    "analyze_policy(env, policy)\n",
    "\n",
    "print_policy = False\n",
    "if print_policy:\n",
    "    prev_state = (0,0)\n",
    "    for (sidx, state) in enumerate(env.states): \n",
    "        if state in policy and state not in env.goal_states and state not in env.terminal_states: \n",
    "            if state[0] != prev_state[0]:\n",
    "                print(\"\\n\\n\")\n",
    "            print(\"\\nState: \", state)\n",
    "            action = policy[state]\n",
    "            print(\"Action: \", env.action_names[action])\n",
    "            print(\"Q: \", Q[sidx])\n",
    "            prev_state = state\n",
    "\n",
    "\n",
    "# plot policy \n",
    "\n",
    "#plot_policy_bar(toy, policy)\n",
    "plot_policy_scatter(env, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
